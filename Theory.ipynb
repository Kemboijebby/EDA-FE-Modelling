{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**<br>\n",
    "Precision can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP).\n",
    "\n",
    "So, Precision identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class.\n",
    "\n",
    "Mathematically, precision can be defined as the ratio of TP to (TP + FP).\n",
    "\n",
    "**Recall**<br>\n",
    "Recall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). Recall is also called Sensitivity.\n",
    "\n",
    "Recall identifies the proportion of correctly predicted actual positives.\n",
    "\n",
    "Mathematically, recall can be given as the ratio of TP to (TP + FN).\n",
    "\n",
    "**f1-score**<br>\n",
    "f1-score is the weighted harmonic mean of precision and recall. The best possible f1-score would be 1.0 and the worst would be 0.0. f1-score is the harmonic mean of precision and recall. So, f1-score is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of f1-score should be used to compare classifier models, not global accuracy."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
